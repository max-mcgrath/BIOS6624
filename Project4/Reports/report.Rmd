---
title: "Project 4 Report"
author: |
    | Analyst: Max McGrath
    | Investigator: Nichole Carlson
    | Report generated: `r format(Sys.Date(), '%m/%d/%Y')`
header-includes:
    - \usepackage{setspace}\doublespacing
    - \usepackage{titling}
    - \usepackage{fixmath}
    - \usepackage{amsmath}
    - \usepackage{lscape}
    - \usepackage{gensymb}
    - \newcommand{\blandscape}{\begin{landscape}}
    - \newcommand{\elandscape}{\end{landscape}}
output: pdf_document
indent: true
mainfont: Times New Roman
fontsize: 12pt
geometry: margin=1in
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(knitr.kable.NA = "-")
library(kableExtra)
library(janitor)
setwd("..")
source("Code/5_SimAnalysis1a.R")
source("Code/6_MakeTables1a.R")
source("Code/7_SimAnalysis1b.R")
source("Code/8_MakeTables1b.R")
source("Code/9_SimAnalysis2a.R")
source("Code/10_MakeTables2a.R")
source("Code/11_SimAnalysis2b.R")
source("Code/12_MakeTables2b.R")
source("Code/13_MakePlots.R")
```

\newpage

## Introduction

This study aims to investigate the efficacy of several model selection 
techniques for linear regression across four case scenarios. More specifically,
it aims to evaluate the ability of a set of backwards selection and
regularization techniques to select models that retain significant predictors
and remove insignificant predictors across scenarios with varying sample sizes
and correlation structures between predictors. To do so, simulation was
used to create regression data sets with known underlying models, then each 
model selection technique was applied to each data set to build a model. 
Each model selection technique's performance was then be evaluated using
a number of criteria that quantify the technique's overall ability to identify 
significant predictors and to accurately estimate those predictors' known 
associations with the response.

Based on these criteria, we will evaluate two hypotheses. First, 
backwards selection using AIC will have a higher true positive rate than BIC at
the cost of a higher false positive rate, as BIC generally favors more
parsimonious models than AIC. Second, backwards selection 
methods will generally perform "better" than regularization methods, as 
regularization methods are typically preferred for prediction and situations 
where the number of predictors is greater than the sample size rather than
the situation under consideration in which inference is the goal and
the set of predictors is relatively small.

## Model Notation

For all case scenarios, 1000 data sets will be simulated from the model
$$
y_j=\beta_1x_{1,j}+\beta_2x_{2,j}+\cdots+\beta_{20}x_{20,j}+\epsilon_j.
$$
where 
$\{\beta_1,...,\beta_5\}=\{\frac{1}{6},\frac{1}{3},\frac{1}{2},\frac{2}{3},\frac{5}{6}\}$, 
$\beta_6,\ldots,\beta_{20}$ are equal to 0, $\epsilon_j\sim{N(0,1)}$, $X_p\sim{N(0,1)}$, and 
$j=1,2,\ldots,N$. The
data sets for the four case scenarios then differ based on sample size, $N$, 
and correlation 
structure as follows: (1a) $N=250$ and all predictors are independent, (1b)
$N=250$ and predictors have an exchangeable correlation coefficient of $0.4$,
(2a) $N=500$ and all predictors are independent, and (2b)
$N=500$ and predictors have an exchangeable correlation coefficient of $0.4$.
Predictors with a non-zero known coefficient will be referred to as significant
predictors, and those with a known zero coefficient will be referred to as
insignificant.

## Methods

For each data set, model selection techniques were applied including
three backwards selection techniques and two regularization methods, and records
were created of which predictors were retained in each final model along with estimated 
coefficients and any other relevant statistics. For backwards selection,
we evaluated three techniques with differing predictor retention criteria. For the
first, predictors were removed one at a time according to highest t-test p-value
until all predictors have a p-value under $0.15$. For the second and third,
we used Akaike information criterion
(AIC) and Bayesian information criterion (BIC) to iteratively reduce the model until
the respective information criteria no longer decreases as predictors are 
removed. For p-value backwards selection the `ols_step_backwards_p()` function
was used from the `olsrr` package. For AIC and BIC backwards selection, the
function `stepwise()` was used from the `RcmdrMisc` R package.

For regularization, we applied LASSO and Elastic Net (ENet), each with both
regularization parameter fixed at $0.2$ and with regularization parameters selected using 
5-fold cross-validation across a range of parameter values from $10^{10}$ to $10^{-2}$. 
For Elastic Net, the penalization mixing parameter will
be fixed at $0.5$. Parameters will then be be considered retained if their 
coefficient estimate is greater than zero and removed if their coefficient estimate
is equal to zero. For full details on LASSO and ENet see Tibsheranti (1996) and
Zou and Hastie (2004). For LASSO and ENet, the `cv.glment()` and `glmnet()` functions
were used from the `glmnet` package.

### Simulation

For each case scenario and simulation, $X_p, p=1,\ldots,20$ was drawn from a $N(0,1)$ 
distribution. Then, in the cases where predictors are correlated an additional
step was added where the $X$ matrix is multiplied by the Cholesky decomposition
of a $20\times20$ matrix with $1$'s along the diagonal and $0.4$ everywhere else to
induce exchangeable correlation. The response for each observation is then drawn
from a normal
distribution with the linear combination of the predictors and coefficients as
the mean and $1$ as the standard deviation. All data was generated using the 
`genData()` function from the `hdrm` R package.

Then, for each data set, a model was selected using each technique, then the 
model was refit using OLS using only the retained predictors. Let $I_R\{X_{p,i}\}$
equal 1 if predictor $X_p,p=1,\ldots,20$ was retained for simulation 
$i=1,2,\ldots,1000$ and 0 
otherwise. Then, each selection technique was evaluated for each case scenario 
according to the following criteria:

1. True positive rate for each significant predictor $p=1,\ldots,5$ where 
$TPR_{p}=\frac{\sum_{i=1}^{1000}I_R\{X_{p,i}\}}{1000}$

2. Total true positive rate where
$TTPR=\frac{\sum_{i=1}^{1000}\sum_{p=1}^{5}I_R\{X_{p,i}\}}{5000}$

3. False positive rate where 
$FPR=\frac{\sum_{i=1}^{1000}\sum_{p=6}^{20}I_{R}\{{X_{p,i}}\}}{15000}$

4. False discovery rate where 
$FDR=\sum_{i=1}^{1000}\frac{\sum_{p=6}^{20}I_{R}\{{X_{p,i}}\}}{\sum_{p=1}^{20}I_{R}\{{X_{p,i}}\}}$

5. Coefficient bias for each significant predictor $\beta_p, p=1,\ldots,5$ where
$Bias_p=\sum_{i=1}^{1000}[\frac{\hat{\beta_p}I_{R}\{X_{p,i}\}}{I_{R}\{X_{p,i}\}}]-\beta_p$

6. Confidence interval coverage for each significant predictor 
$\beta_p, p=1,\ldots,5$ where
$CIC_p=\sum_{i=1}^{1000}\frac{I_{CI}\{\hat{\beta}_{p,i}\}}{I_{R}\{X_{p,i}\}}$ where
$I_{CI}\{\hat{\beta}_{p,i}\}$ equals 1 if the 95% confidence interval for $\hat{\beta_p}$
contains $\beta_p$ for simulation $i$.

7. Conditional Type I error rate where
$T1ER=1-\sum_{i=1}^{1000}\sum_{p=1}^{5}\frac{I_{T1}\{{X_{p,i}}\}}{I_{R}\{{X_{p,i}}\}}$
where $I_{T1}\{X_{p,i}\}$ equals 1 if the coefficient estimate for predictor $p$ is 
statistically significant according to a Wald test for simulation $i$

8. Conditional Type II error rate where
$T2ER=\sum_{i=1}^{1000}\sum_{p=6}^{20}\frac{I_{T2}\{{X_{p,i}}\}}{I_{R}\{{X_{p,i}}\}}$
where $I_{T2}\{X_{p,i}\}$ equals 1 if the coefficient estimate for predictor $p$ is 
statistically significant according to a Wald test for simulation $i$

Note that for regularization methods, evaluation metrics 5 through 8 were all
calculated using the models refit with OLS following predictor selection. No
data other than coefficient retention was used from the regularized model fits.

## Results

Figure 1 provides plots of the TTPR, FPR, FDR, Types I and II error rates, and
the average bias by case scenario and model selection technique, Figure 2 
provides plots of bias, 95% confidence interval coverage, and TPR by coefficient,
case scenario, and model selection technique, and Tables 1 through 4 provide
a detailed look at all simulation evaluation metrics. In the discussion which
follows, regularization will generally refer to LASSO with CV and ENet with CV,
as regularization with fixed regularization parameters is contrary to standard
practice and of secondary interest. However, all tables and figures report
values for all seven model selection techniques considered.

A higher TTPR indicates that a selection technique retains known significant
predictors at a greater rate, and from Figure 1 and the tables, we see that
regularization generally performs better than backwards selection in terms of
TTPR. LASSO with CV and ENet with CV have higher TTPR than backwards selection
techniques across all case scenarios. This difference is more pronounced in 
scenarios where correlation is induced (1b and 2b) and when sample size is 
smaller (1a and 1b). Among the backwards selection techniques, p-value selection
and AIC selection perform relatively the same, while BIC selection has notably
lower TTPR across all case scenarios. TTPR rates are all relatively high, with
the majority of methods having a TTPR above 0.95 for most or all cases 
scenarios. The lowest TTPR is BIC for cases scenario 1b, where it has a TTPR of
0.880. From Figure 2 we see that TPR by coefficient
was nearly 1 for all predictors other than $X_1$, and as such TTPR and TPR for
$X_1$ are nearly identical.

These TTPR trends were reversed for FPR and FDR. Lower FPR and FDR generally
reflects a model selection techniques ability to remove insignificant 
predictors, and in this regard backwards selection performed better than 
regularization. Across all case scenarios, backwards selection techniques
had lower FPR and FDR than regularization techniques, with BIC having the lowest
FPR and FDR among backwards selection techniques. However, we see far greater
variation in FPR relative to TTPR, as across case scenarios BIC remains consistently
near 0.025 while ENet with CV reaches a high of 0.504 for case scenario 2a.

For conditional Type I error, we see fairly consistent behavior across methods, with 
backwards selection using p-values and AIC generally having the lowest Type 1
error rate,
followed by BIC and the regularization methods. However, for Case 2a we
see regularization marginally outperforming backwards selection methods. In
contrast, we see that for scenario 1b backwards selection greatly outperforms
regularization, with backwards selection having conditional Type I error rates below
0.05 and regularization methods all above 0.10.

For conditional Type II error, we see that for case scenarios 1a, 1b, and 2b, 
regularization outperforms backwards selection. However, for case scenario
2a, we see AIC and BIC backwards selection notably outperform regularization
methods (but backward selection using p-values performs worse).

For coefficient bias, we see somewhat mixed results across case scenarios
and methods, but generally bias is higher for regularization selection over
backwards selection, and coefficient bias was consistently small relative to
coefficient size (between -0.03 and 0.03). Bias was generally greatest for 
$\hat{\beta}_1$\ (as would be expected given its effect size), although
regularization methods did have visibly higher biases for $X_2,\ldots,X_5$
for case scenarios 1b and 2b.

For coefficient confidence interval coverage, we see fairly consistent
performance across methods and case scenarios with most estimates ranging 
from 0.93 to 0.97 and little in the way of discernible trends between
methods and case scenarios.

## Discussion

The most notable trend across methods in terms of predictor retention was the
trade off between TPR and FPR. Generally, methods with higher TPR also had
higher FPR. However, these tradeoffs were not always proportional. 
For AIC and BIC, the hypothesized trade off between true positive rate and 
false positive rate is as expected: AIC generally has higher for both. However, we see
that across the four case scenarios the change in TPR between AIC and BIC and
that of FPR is notably different. For BIC, all scenarios had a fairly
consistent FPR (0.015 - 0.024) while for TTPR we see a greater range across case
scenarios (0.880 to 0.980). This is noteworthy because it is indicative that the
trade-off may be
more beneficial for BIC in certain scenarios. Specifically, BIC sees notable
improvements in FPR and only marginal disadvantages in TPR in situations with
greater sample size (2a and 2b), while the difference in FPR is more pronounced
in situations with smaller sample sizes (1a and 1b).

Similarly, while regularization methods all had extremely high TPR, this benefit
comes at the cost of a much higher FPR. Gains in TPR for regularization over
AIC selection were less than 0.025 in all scenarios, whereas FPR was more than
doubled for regularization methods compared to AIC selection across cases scenarios,
noting that this problem is even worse for situations without correlated predictors.
This somewhat confirms our hypothesis that backwards selection generally performs
"better" than regularization, but there are caveats.

Interestingly, the introduction of correlation and reduction of sample size
appears to
have a compounding effect in the reduction of TPR for backwards selection.
However, for regularization methods, only sample size appeared to reduce TPR,
as TPR was roughly the same for 1a and 1b (and 2a and 2b),
indicating that regularization's ability to retain significant predictors may
be more robust when predictors are correlated. This is contrasted by FPR,
where backwards selection
techniques had highly consistent FPR across case scenarios, whereas
regularization selection had notably lower FPR for scenarios with correlated
predictors. So, there is the potential that in situations with correlated
predictors and in which the importance of significant predictor retention far exceeds
that of insignificant predictor removal regularization may be preferable.

## Limitations and Future Work

There are several limitations to this analysis that provide insight into further
cases scenarios and methodologies to consider. First, 
using LASSO and ENet with pre-specified regularization parameters is detached from
standard practice and is unlikely to reflect real world uses of those methods.
Similarly, refitting the models "selected" via regularization using OLS is 
likely problematic and mischaracterizes the statistical intentions of those 
methods. A more statistically sound method for assessing significance and CI's
would be more appropriate for regularization methods and would likely alter
the results obtained.

Second, as previously noted TTPR and TPR for $X_1$ were essentially the same
stemming from the incredibly consistent TPR of 1 for all other predictors. As
such, the effect sizes investigated in this analysis may have been too large, and
future analysis should consider using smaller effect sizes as it may provide 
greater insight into the efficacy of the considered model selection approaches.

Lastly, the correlation structure induced in the predictors for case scenarios
1b and 2b was fairly simplistic and is unlikely to be representative of real
world correlations. Future works should consider inducing more realistic
correlation scenarios to more thoroughly elucidate the differences
in model selection techniques.

\newpage

## Reproducibility

The code used to generate this analysis is available on GitHub 
at https://github.com/BIOS6624-UCD/bios6624-MaxMcGrath/tree/main/Project4
. The `Background` folder contains information pertinent to understanding the
analysis but unnecessary for reproducing it. The `Code` folder contains thirteen
files: `1_Case1a.R`, `2_Case1b.R`, `3_Case2a.R`, `4_Case2b.R`,
`5_SimAnalysis1a.R`, `6_MakeTables1a.R`,
`7_SimAnalysis1b.R`, `8_MakeTables1b.R`,
`9_SimAnalysis2a.R`, `10_MakeTables2a.R`,
`11_SimAnalysis2b.R`, `12_MakeTables2b.R`, and `13_MakePlots.R`. 
To run the complete analysis, each
script should be run in the order of the number prefixing its filename. The
last directory, `Report`, contains the RMarkdown file `report.Rmd` which may
be used to generate this report (note that it also depends on the aforementioned
scripts).

The complete details of the `R` version, package versions, and
machine details for the instance which generated this report are provided below.

```{r, echo = FALSE}
sessionInfo()
```

\newpage

## Works Cited

| Tibshirani, Robert. “Regression Shrinkage and Selection via the Lasso.” Journal of the
|       Royal Statistical Society. Series B (Methodological) 58, no. 1 (1996): 267–88.
|       http://www.jstor.org/stable/2346178.
| Zou, H. and Hastie, T. (2005), Regularization and variable selection via the elastic net. 
|       Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67: 301-320.
|       https://doi.org/10.1111/j.1467-9868.2005.00503.x

\newpage

## Appendix - Tables and Figures

```{r, fig.cap = "Grouped coefficient evaluation criteria for model selection techniques by technique and case scenario", fig.height = 8.5, fig.width = 9, echo=FALSE}
modelSummaryPlots
```

\newpage

\blandscape

```{r, fig.cap = "Known significant coefficient evaluation criteria by technique, case scenario, and coefficient. Here, pVal refers to backwards selection by p-value, LAS to LASSO, LCV to LASSO with cross-validation, EN to elastic net, and ENCV to Elastic Net with cross-validation", fig.height = 7, fig.width = 11, echo = FALSE}
coefPlots
```

\elandscape

\newpage

```{r, echo = FALSE}
kable(bind_rows(modelDF, coefDF), booktabs = TRUE, 
      caption = "Model Selection Technique Summary - Case 1a", 
      digits = 3,
      col.names = c("", "P-Val", "AIC", "BIC", "LASSO",
                "LASSO with CV", "Elastic Net", "EN with CV"),
      escape = FALSE) %>%
    kable_styling(latex_options = c("scale_down")) %>%
    pack_rows("Full Model", 1, 5, escape = FALSE, bold = FALSE) %>%
    pack_rows("$\\beta_1=1/6$", 6, 8, escape = FALSE) %>%
    pack_rows("$\\beta_2=1/3$", 9, 11, escape = FALSE) %>%
    pack_rows("$\\beta_3=1/2$", 12, 14, escape = FALSE) %>%
    pack_rows("$\\beta_4=2/3$", 15, 17, escape = FALSE) %>%
    pack_rows("$\\beta_5=5/6$", 18, 20, escape = FALSE) %>%
    column_spec(c(1), border_right = TRUE, width = "1.2in")  %>%
    add_header_above(c(" " = 1, "Backwards Selection" = 3, "Regularization" = 4))
```

\newpage

```{r, echo = FALSE}
kable(bind_rows(modelDF1b, coefDF1b), booktabs = TRUE, 
      caption = "Model Selection Technique Summary - Case 1b", 
      digits = 3,
      col.names = c("", "P-Val", "AIC", "BIC", "LASSO",
                "LASSO with CV", "Elastic Net", "EN with CV"),
      escape = FALSE) %>%
    kable_styling(latex_options = c("scale_down")) %>%
    pack_rows("Full Model", 1, 5, escape = FALSE, bold = FALSE) %>%
    pack_rows("$\\beta_1=1/6$", 6, 8, escape = FALSE) %>%
    pack_rows("$\\beta_2=1/3$", 9, 11, escape = FALSE) %>%
    pack_rows("$\\beta_3=1/2$", 12, 14, escape = FALSE) %>%
    pack_rows("$\\beta_4=2/3$", 15, 17, escape = FALSE) %>%
    pack_rows("$\\beta_5=5/6$", 18, 20, escape = FALSE) %>%
    column_spec(c(1), border_right = TRUE, width = "1.2in")  %>%
    add_header_above(c(" " = 1, "Backwards Selection" = 3, "Regularization" = 4))
```

\newpage

```{r, echo = FALSE}
kable(bind_rows(modelDF2a, coefDF2a), booktabs = TRUE, 
      caption = "Model Selection Technique Summary - Case 2a", 
      digits = 3,
      col.names = c("", "P-Val", "AIC", "BIC", "LASSO",
                "LASSO with CV", "Elastic Net", "EN with CV"),
      escape = FALSE) %>%
    kable_styling(latex_options = c("scale_down")) %>%
    pack_rows("Full Model", 1, 5, escape = FALSE, bold = FALSE) %>%
    pack_rows("$\\beta_1=1/6$", 6, 8, escape = FALSE) %>%
    pack_rows("$\\beta_2=1/3$", 9, 11, escape = FALSE) %>%
    pack_rows("$\\beta_3=1/2$", 12, 14, escape = FALSE) %>%
    pack_rows("$\\beta_4=2/3$", 15, 17, escape = FALSE) %>%
    pack_rows("$\\beta_5=5/6$", 18, 20, escape = FALSE) %>%
    column_spec(c(1), border_right = TRUE, width = "1.2in")  %>%
    add_header_above(c(" " = 1, "Backwards Selection" = 3, "Regularization" = 4))
```

\newpage

```{r, echo = FALSE}
kable(bind_rows(modelDF2b, coefDF2b), booktabs = TRUE, 
      caption = "Model Selection Technique Summary - Case 2b", 
      digits = 3,
      col.names = c("", "P-Val", "AIC", "BIC", "LASSO",
                "LASSO with CV", "Elastic Net", "EN with CV"),
      escape = FALSE) %>%
    kable_styling(latex_options = c("scale_down")) %>%
    pack_rows("Full Model", 1, 5, escape = FALSE, bold = FALSE) %>%
    pack_rows("$\\beta_1=1/6$", 6, 8, escape = FALSE) %>%
    pack_rows("$\\beta_2=1/3$", 9, 11, escape = FALSE) %>%
    pack_rows("$\\beta_3=1/2$", 12, 14, escape = FALSE) %>%
    pack_rows("$\\beta_4=2/3$", 15, 17, escape = FALSE) %>%
    pack_rows("$\\beta_5=5/6$", 18, 20, escape = FALSE) %>%
    column_spec(c(1), border_right = TRUE, width = "1.2in")  %>%
    add_header_above(c(" " = 1, "Backwards Selection" = 3, "Regularization" = 4))
```



